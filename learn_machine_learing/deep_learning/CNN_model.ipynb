{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络之模型\n",
    "\n",
    "+ 模型演变：AlexNet、VGG、ResNet、Inception、MobileNet\n",
    "\n",
    "+ 模型对比和选择\n",
    "\n",
    "参考：https://blog.csdn.net/qq_34759239/article/details/79034849\n",
    "\n",
    "### 不同网络结构的差异\n",
    "\n",
    "+ 不同的网络结构解决的问题不同\n",
    "\n",
    "+ 不同的网络结构使用的技巧不同 \n",
    "\n",
    "+ 不同的网络结构应用的场景不同\n",
    "\n",
    "### 模型的进化\n",
    "\n",
    "从简单到更深更宽 ： AlexNet 到 VGGNet \n",
    "\n",
    "更多模型结构 ： VGG 到 InceptionNet/ResNet\n",
    "\n",
    "优势组合 ： Inception + Res = InceptionResNet\n",
    "\n",
    "自我（强化）学习 ： NASNet \n",
    "\n",
    "实用 ： MoblieNet\n",
    "\n",
    "### 模型：AlexNet\n",
    "\n",
    "**网络结构**\n",
    "\n",
    "结构及参数如图所示，是8层网络结构（忽略激活，池化，LRN，和dropout层）,有5个卷积层和3个全连接层，第一卷积层使用大的卷积核，大小为11 * 11，步长为4，第二卷积层使用5 * 5的卷积核大小，步长为1，剩余卷积层都是3 * 3的大小，步长为1。激活函数使用ReLu（虽然不是他发明，但是他将其发扬光大），池化层使用重叠的最大池化，大小为3 * 3，步长为2。在全连接层增加了dropout，第一次将其实用化。\n",
    "\n",
    "![alt_text](./img/alexnet.png)\n",
    "\n",
    "**网络特点**\n",
    "\n",
    "+ 首次使用Relu\n",
    "+ 2-GPU并行结构，大大提高了运行时间\n",
    "+ 1,2,5卷积层后跟随max-pooling层\n",
    "+ 两个全连接层上使用了dropout，增强模型的泛化能力（用在全连接层上原因：全连接层参数占全部参数数目的大部分，容易过拟合）\n",
    "+ 数据增强，图片随机采用\n",
    "+ dropout=0.5\n",
    "+ Batch size = 128\n",
    "+ SGD momentum = 0.9\n",
    "+ Learning rate = 0.01，过一定次数降低为1/10\n",
    "+ 7个CNN做ensemble：18.2%->15.4%\n",
    "    \n",
    "### 模型：VGGNet\n",
    "\n",
    "**网络结构**\n",
    "\n",
    "VGG16的网络结构，共16层（不包括池化和softmax层），所有的卷积核都使用3 * 3的大小，池化都使用大小为2 * 2，步长为2的最大池化，卷积层深度依次为64 -> 128 -> 256 -> 512 ->512。\n",
    "\n",
    "![alt_text](./img/VGGNet.png)\n",
    "\n",
    "**网络特点**\n",
    "+ 更深的神经网络：把网络层数加到了16-19层（不包括池化和softmax层），而AlexNet是8层结构。\n",
    "+ 将卷积层提升到卷积块的概念。卷积块有2~3个卷积层构成，使网络有更大感受野的同时能降低网络参数，同时多次使用ReLu激活函数有更多的线性变换，学习能力更强\n",
    "+ 多使用3 * 3的卷积核：2个3 * 3卷积层 = 一层5 * 5卷积层。2层比1层更多一次非线性变换，可以学到更多东西，参数变得更少。\n",
    "+ 多使用1 * 1的卷积核，起到降维的作用。\n",
    "+ 每经过一个pooling层，通道数目翻倍。（信息丢失问题）\n",
    "\n",
    "### 模型：ResNet\n",
    "\n",
    "**加深层次存在问题：**模型深度达到某个程度后继续加深导致训练集准确率下降。\n",
    "\n",
    "**问题解决：**假设：深层网络更难优化而非深层网络学不到东西\n",
    "+ 深层网络至少可以和浅层网络持平\n",
    "+ y=x，虽然增加了深度，但误差不会增加 \n",
    "\n",
    "**网络结构**\n",
    "\n",
    "![alt_text](./img/ResNet1.png)\n",
    "\n",
    "+ Identity部分是恒等变换\n",
    "+ F(x)是残差学习\n",
    "\n",
    "上图为残差神经网络的基本模块（专业术语叫残差学习单元）通过不断堆叠这个基本模块，就可以得到最终的ResNet模型，理论上可以无限堆叠而不改变网络的性能。下图为一个34层的ResNet网络。\n",
    "+ 先用一个普通的卷积层，stride=2\n",
    "+ 再经过一个 max_pooling\n",
    "+ 再经过残差结构\n",
    "+ 没有中间的全连接层，直接到输出\n",
    "\n",
    "![alt_text](./img/ResNet.png)\n",
    "\n",
    "**网络特点**\n",
    "+ 残差结构使网络需要学习的知识变少，容易学习\n",
    "+ 残差结构使每一层的数据分布接近，容易学习\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
