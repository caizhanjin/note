{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 激活函数\n",
    "\n",
    "总结：激活函数是用来加入非线性因素的，实现去线性化，提高神经网络对模型的表达能力，解决线性模型所不能解决的问题。\n",
    "\n",
    "特征：具有单调性，非线性函数。\n",
    "\n",
    "+ Sigmoid\n",
    "+ tanh\n",
    "+ ReLU\n",
    "+ Leaky ReLU\n",
    "+ Maxout\n",
    "+ ELU\n",
    "\n",
    "## Sigmoid\n",
    "+ 输入非常大或非常小时没有梯度\n",
    "+ 输出均值非0\n",
    "+ Exp计算复杂，速度就会变慢\n",
    "+ 梯度消失\n",
    "![alt_text](./img/sigmod.png)\n",
    "\n",
    "## Tanh\n",
    "+ 依旧没有梯度\n",
    "+ 输出均值是0\n",
    "+ 计算也是复杂\n",
    "![alt_text](./img/tanh.png)\n",
    "\n",
    "## ReLU \n",
    "+ 不饱和（梯度不会过小）\n",
    "+ 计算量小\n",
    "+ 收敛速度快\n",
    "+ 输出均值非0\n",
    "+ Dead ReLU：一个非常大的梯度流过神经元，不会再对数据有激活现象了（小于零时，梯度得不到更新）\n",
    "![alt_text](./img/relu.png)\n",
    "\n",
    "## Leaky-ReLU\n",
    "+ 解决ReLU的问题\n",
    "![alt_text](./img/leaky_relu.png)\n",
    "\n",
    "## ELU\n",
    "+ 均值更接近0\n",
    "+ 小于0时，计算量大\n",
    "![alt_text](./img/elu.png)\n",
    "\n",
    "## Maxout\n",
    "+ ReLU 的泛化版本\n",
    "+ 没有deal relu\n",
    "+ 但是参数会翻倍\n",
    "\n",
    "## 使用激活函数技巧\n",
    "+ Relu小心设置learning rate\n",
    "+ 不要使用sigmoid（过大、慢）\n",
    "+ 使用Leaky ReLU、maxout、ELU\n",
    "+ 可以试试tanh，但不要抱太大期望（计算量大）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
