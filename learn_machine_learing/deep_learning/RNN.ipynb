{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循环神经网络RNN\n",
    "\n",
    "+ 理解循环神经网络\n",
    "+ 多层网络与双向网络\n",
    "+ 模型：长短线期记忆网络（LSTM）\n",
    "\n",
    "### 循环神经网络解决那些问题\n",
    "普通神经网络一般是一个输入对应一个输出。而循环神经网络主要解决**序列式问题**。\n",
    "\n",
    "+ 1对多：比如图片生成描述。\n",
    "+ 多对1：文本分类（文本情感分析）。\n",
    "+ 多对多：encoding-decoding，机器翻译、视频解说。\n",
    "\n",
    "### RNN的特点\n",
    "+ 维护一个状态作为下一步的额外输入\n",
    "+ 每一步使用同样的激活函数和参数\n",
    "+ 损失函数计算：每个单元的状态都会输入计算，会分块\n",
    "\n",
    "## 分类\n",
    "正向传播：\n",
    "\n",
    "反向传播：\n",
    "+ 激活函数Tanh输出在-1和1之间\n",
    "+ 梯度消失\n",
    "+ 较远的步骤梯度贡献很小\n",
    "+ 切换其他激活函数后，而不是Tanh后，可能会导致爆炸\n",
    "\n",
    "多层神经网络：\n",
    "+ 底层输入作为高层的输出\n",
    "+ 同层之间依旧递归\n",
    "+ 增加网络拟合能力\n",
    "+ 一般隐层维度递增\n",
    "    + 64 - 128 - 256\n",
    "\n",
    "双向网络：\n",
    "+ 另一条以未来状态为输入\n",
    "+ 两个状态合并后进入输入层\n",
    "+ 进一步提高表达能力\n",
    "+ 无法实时输出结果，需要把内容全部输入后才会有\n",
    "\n",
    "### 为什么需要LSTM\n",
    "+ 普通RNN的信息不能长久传播（存在于理论上）\n",
    "+ 这时，引入了选择性机制，分为：选择性输出、选择性输入、选择性遗忘。\n",
    "+ 选择性是通过门来实现的，例：Sigmoid函数：[0, 1, 0.5]\n",
    "\n",
    "### LSTM结构\n",
    "https://blog.csdn.net/gzj_1101/article/details/79376798\n",
    "\n",
    "![alt_text](./img/rnn1.jpg)\n",
    "\n",
    "如上图所示，从左到右：\n",
    "+ 遗忘门：忘记一些信息，如，新的语句有新的主语，需要忘记掉之前的主语\n",
    "+ 输入门：决定输入那些信息，如，需不需要输入主语的性别信息\n",
    "+ 输出门：决定如何输出信息，如，动词该用单数形式还是复数形式\n",
    "+ C t-1隐含状态传递\n",
    "+ C t 当前状态\n",
    "\n",
    "### 文本分类\n",
    "**CNN文本分类：**\n",
    "对于文本分类，CNN也是可以用的，句子的长度有长有短，需要做一些变换，长的取前n代表性的词，而短的做padding。如果句子很长就不生效了。\n",
    "+ 一维卷积：\n",
    "    + 应用在时间维度上：从句子的开头到结尾\n",
    "    + Embedding长度就是通道数目\n",
    "    + 多种层次的卷积核，就是不同大小的卷积核，然后得到多个结果\n",
    "+ 池化： 在时间层次上pooling，每个卷积核信息的汇总\n",
    "+ 全连接\n",
    "\n",
    "**CNN和RNN的对比：**\n",
    "+ CNN不能完美地解决序列式问题\n",
    "+ CNN卷积相当于N-gram，只能看到n个词，LSTM提取更长的依赖\n",
    "+ 双向RNN会增强效果\n",
    "+ CNN模型并行程度高，更大；因为RNN需要上一次输出才能计算下一个\n",
    "\n",
    "**CNN和RNN结合：R-CNN文本分类**\n",
    "+ 双向RNN提取特征\n",
    "+ CNN进一步抽取\n",
    "+ Max-pooling\n",
    "+ 全连接层\n",
    "\n",
    "**Embedding压缩**\n",
    "相同点：Pre-train的embedding，如果Embedding层次参数过大，模型表达能力过强就会无法使用，过拟合。\n",
    "\n",
    "### 文本分类三个例子：在example中\n",
    "+ TensorFlow中，使用LSTM实现文本分类\n",
    "+ TensorFlow中，实现LSTM网络结构-细节化\n",
    "+ TensorFlow中，使用CNN实现文本分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
