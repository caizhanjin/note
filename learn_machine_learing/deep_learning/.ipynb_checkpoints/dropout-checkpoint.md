# 正则化
机器学习中的一个核心问题是设计不仅在训练集上误差小，而且在新样本上泛化能力好的算法。许多机器学习算法都需要采取相应的策略来减少测试误差，这些策略被统称为正则化。
*正则化是防止过拟合最常用的方法。*

https://segmentfault.com/a/1190000015486067


# Dropout

+ 出现原因
+ 原理解释
+ 工作流程

参考链接：https://blog.csdn.net/program_developer/article/details/80737724

### 出现原因
在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。

过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。

综上所述，训练深度神经网络的时候，总是会遇到两大缺点：
+ 容易过拟合
+ 费时

Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。

### 原理解释
Dropou简单来说：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。如下图：
![alt_text](./img/dropout1.png)

**组合解析**：每次dropout都相当于训练了一个子网络，最后的结果相当于很多子网络组合。

**动机解析**：消除了神经单元之间的依赖，增强泛化能力。

**数据解析**：对于dropout后的结果，总能找到一个样本与其对应。作用相当于数据增强。

### 工作流程

```{.python .input}

```
