{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化\n",
    "机器学习中的一个核心问题是设计不仅在训练集上误差小，而且在新样本上泛化能力好的算法。许多机器学习算法都需要采取相应的策略来减少测试误差，这些策略被统称为正则化。\n",
    "*正则化是防止过拟合最常用的方法。*\n",
    "\n",
    "https://segmentfault.com/a/1190000015486067\n",
    "\n",
    "\n",
    "# Dropout\n",
    "\n",
    "+ 出现原因\n",
    "+ 原理解释\n",
    "+ 工作流程\n",
    "\n",
    "参考链接：https://blog.csdn.net/program_developer/article/details/80737724\n",
    "\n",
    "### 出现原因\n",
    "在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象。在训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。\n",
    "\n",
    "过拟合是很多机器学习的通病。如果模型过拟合，那么得到的模型几乎不能用。为了解决过拟合问题，一般会采用模型集成的方法，即训练多个模型进行组合。此时，训练模型费时就成为一个很大的问题，不仅训练多个模型费时，测试多个模型也是很费时。\n",
    "\n",
    "综上所述，训练深度神经网络的时候，总是会遇到两大缺点：\n",
    "+ 容易过拟合\n",
    "+ 费时\n",
    "\n",
    "Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。\n",
    "\n",
    "### 原理解释\n",
    "Dropou简单来说：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。如下图：\n",
    "![alt_text](./img/dropout1.png)\n",
    "\n",
    "**组合解析**：每次dropout都相当于训练了一个子网络，最后的结果相当于很多子网络组合。\n",
    "\n",
    "**动机解析**：消除了神经单元之间的依赖，增强泛化能力。\n",
    "\n",
    "**数据解析**：对于dropout后的结果，总能找到一个样本与其对应。作用相当于数据增强。\n",
    "\n",
    "### 工作流程\n",
    "\n",
    "```{.python .input}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
