# 神经网络入门

### 神经元

最小的神经网络。
![Alt text](./img/673793-20151228134016120-1091351096.jpg)


### 神经元例子：Softmax回归（多分类模型）

![Alt text](./img/1503286054122579.png)
![Alt text](./img/1503286124770352.png)

使用公式表示就是：
![Alt text](./img/1503286210614595.png)

其中：
+ X：输入，就是特征
+ W：权重
+ solfmax：激活函数
+ b：偏置
+ y：输出
+ loss：目标函数，就是损失函数，常见：平方差损失、交叉熵损失（适合多分类）

### 神经网络

多层神经元 -> 神经网络


### 神经网络训练
神经网络的目的就是：**调整参数使模型在训练集上的损失函数最小**。

+ 下山算法：
    + 找到方向
    + 走一步

+ 神经网络中：梯度下降算法
θ = θ−α * (∂*J(θ)\∂θ)
    (∂*J(θ)\∂θ) : 方向
    α ： 学习率
    
### 神经网络训练优化
原本的做法：
+ 每次都在这个数据集上计算loss和梯度
    + 计算量大
    + 可能内存承载不了
+ 梯度方向确定的时候，仍然是每次都走一个单位步长
    + 每次都重新计算梯度，太慢
    
优化的做法：
+ 随机梯度下降
    + 每次只使用一个样本
+ Mini-Batch梯度下降（一般会使用这个）
    + 每次使用小部分数据进行训练
存在问题：
+ 梯度下降存在震荡的问题
+ 梯度下降存在局部极值（会出现多个解）和saddle point（导数为0的位置）的问题

解决上面问题：
+ 动量梯度下降SGD
    + 开始训练时，积累动量，加速训练
    + 局部极值附近震荡时，梯度为0，由于动量，跳出陷阱
    + 梯度改变方向的时候，动量缓解动荡















































