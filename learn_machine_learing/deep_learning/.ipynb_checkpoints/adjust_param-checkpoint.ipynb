{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调参技巧\n",
    "\n",
    "+ 各种优化算法Optimizer\n",
    "+ [激活函数](./activation_function.ipynb)\n",
    "+ 网络初始化\n",
    "+ 批归一化\n",
    "+ 数据增强\n",
    "+ 其他调参技巧\n",
    "\n",
    "\n",
    "## 各种优化算法Optimizer\n",
    "https://www.cnblogs.com/guoyaohua/p/8542554.html\n",
    "\n",
    "#### 随机梯度下降\n",
    "https://www.cnblogs.com/pinard/p/5970503.html\n",
    "\n",
    "+ 定义：把求得的各个参数的偏导数以向量的形式写出来，就是梯度。从几何意义上讲，就是函数变化增加最快的地方。\n",
    "+ 局部极值，只有凹函数时才确保是最优解，有时候初始值非常关键\n",
    "+ Saddle point问题\n",
    "+ 分类：BGD，SGD，MBGD\n",
    "+ 算法调优：\n",
    "    + 1. 算法的步长（Learning rate）选择。\n",
    "    + 2. 算法参数的初始值选择。 \n",
    "    + 3. 归一化。\n",
    "\n",
    "#### 动量下降\n",
    "\n",
    "随机梯度下降和动量下降存在问题：会很难学习到稀疏数据上的信息\n",
    "+ 受初始学习率影响很大\n",
    "+ 每一个维度的学习率一样\n",
    "\n",
    "#### AdaGrad算法\n",
    "+ 前期，分母regularrizer较小，放大梯度\n",
    "+ 后期，分母regularrizer较大，缩小梯度\n",
    "+ 梯度随训练次数降低\n",
    "+ 每个分量有不同的学习率\n",
    "+ 对稀疏的数据表现很好，减少学习率的手动调节\n",
    "+ 超参数设定值：一般η选取0.01\n",
    "\n",
    "缺点：\n",
    "+ 学习率设置太大时，会导致分母regularrizer较大影响过于敏感\n",
    "+ 后期时，regularrizer累积值太大，提前结束训练\n",
    "\n",
    "#### RMSProp算法\n",
    "+ AdaGrad的变种\n",
    "+ 由累积平方梯度变为平均平方梯度\n",
    "+ 解决了后期提前结束的问题\n",
    "\n",
    "#### Adam算法\n",
    "+ 相当于RMSprop + Momentum\n",
    "+ 所有上述算法都会用learning_rate来做参数\n",
    "+ 实践表明，Adam比其他适应性学习方法效果好\n",
    "+ Adam比较有效的参数：\n",
    "    + Beta1=0.9\n",
    "    + Beta2=0.999\n",
    "    + Learning_rate=1e-3 or 5e-4\n",
    "    \n",
    "#### 如何选择优化算法\n",
    "+ 对于稀疏数据，使用学习率可自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。\n",
    "+ RMSprop, Adadelta, Adam 在很多情况下的效果是相似\n",
    "+ 需要训练较深较复杂的网络且需要快速收敛时，推荐使用Adam。Adam 在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变的稀疏，Adam 比 RMSprop 效果会好。**整体来讲，Adam 是最好的选择。**\n",
    "+ 很多论文会用 SGD。SGD通常训练时间长，最终效果会比较好，但是需要好的初始化和learning rate\n",
    "+ 如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。\n",
    "\n",
    "\n",
    "## [激活函数](./activation_function.ipynb)\n",
    "\n",
    "## 一种优化算法：Batch Normalization(批归一化)\n",
    "https://blog.csdn.net/wfei101/article/details/79997708\n",
    "\n",
    "\n",
    "## 网络初始化\n",
    "如何分析初始化结果好不好？查看初始化各层的激活值分布。\n",
    "\n",
    "+ 1.全部初始化为0，单层网络是可以的。但是多层网络会使梯度消失，链式法则。\n",
    "+ 2.均值为0，方差为0.02的正态分布初始化：tanh问题：高层均值为0，没有梯度\n",
    "+ 3.均值为0，方差为1的正态分布初始化：tanh问题：高层均值为-1、1，已经饱和\n",
    "+ 改善：Xavier-tanh（不适用ReLU）、He-ReLU（改进）\n",
    "\n",
    "## 批归一化？？？\n",
    "+ 每个Batch在每一层上都做归一化\n",
    "+ 为了确保归一化能够起到作用，另设两个参数来逆归一化（没有效果时，就撤回来）\n",
    "\n",
    "## 数据增强\n",
    "[实例：tensorflow之图像数据增强](./data_img_enhance.ipynb)\n",
    "\n",
    "数据层面\n",
    "+ 归一化\n",
    "+ 图像变换：翻转、拉伸、裁剪、变形\n",
    "+ 色彩变换：对比度、亮度\n",
    "+ 多尺度\n",
    "    + 例子：从[256，480]中随机选择一个数，图像短边缩放到这个数，缩放后图像上裁剪(224, 224)\n",
    "    \n",
    "更多方法：\n",
    "+ 拿到更多的数据\n",
    "+ 给神经网络添加层次：逐步增加、容易发现问题，由简单到复杂\n",
    "+ 紧跟最新进展，使用新方法\n",
    "+ 增大训练的迭代次数：增加批次Batch，打乱数据\n",
    "+ 尝试正则化\n",
    "+ 使用更多的GPU来加速训练\n",
    "+ 可视化工具来检查中间状态\n",
    "    + 损失\n",
    "    + 梯度：梯度比较小时，说明训练快结束了\n",
    "    + 准确率：\n",
    "    + 学习率：\n",
    "    \n",
    "可能存在问题：\n",
    "+ 学习不到规律\n",
    "+ 过拟合\n",
    "+ 训练速度慢\n",
    "+ 优化目标写错\n",
    "\n",
    "## 其他调参技巧\n",
    "+ 在标准数据集上训练\n",
    "+ 在小数据集上过拟合\n",
    "+ 数据集分布平衡\n",
    "+ 使用预调整好的稳定模型结构：别人的论文\n",
    "+ Fine-tuning\n",
    "    + 预训练好的网络结构上进行微调\n",
    "+ 加深网络\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
