# 卷积神经网络之模型

+ 模型演变：AlexNet、VGG、ResNet、Inception、MobileNet

+ 模型对比和选择

参考：https://blog.csdn.net/qq_34759239/article/details/79034849

### 不同网络结构的差异

+ 不同的网络结构解决的问题不同

+ 不同的网络结构使用的技巧不同 

+ 不同的网络结构应用的场景不同

### 模型的进化

从简单到更深更宽 ： AlexNet 到 VGGNet 

更多模型结构 ： VGG 到 InceptionNet/ResNet

优势组合 ： Inception + Res = InceptionResNet

自我（强化）学习 ： NASNet 

实用 ： MoblieNet

### 模型：AlexNet

**网络结构**

结构及参数如图所示，是8层网络结构（忽略激活，池化，LRN，和dropout层）,有5个卷积层和3个全连接层，第一卷积层使用大的卷积核，大小为11 * 11，步长为4，第二卷积层使用5 * 5的卷积核大小，步长为1，剩余卷积层都是3 * 3的大小，步长为1。激活函数使用ReLu（虽然不是他发明，但是他将其发扬光大），池化层使用重叠的最大池化，大小为3 * 3，步长为2。在全连接层增加了dropout，第一次将其实用化。

![alt_text](./img/alexnet.png)

**网络特点**

+ 首次使用Relu
+ 2-GPU并行结构，大大提高了运行时间
+ 1,2,5卷积层后跟随max-pooling层
+ 两个全连接层上使用了dropout，增强模型的泛化能力（用在全连接层上原因：全连接层参数占全部参数数目的大部分，容易过拟合）
+ 数据增强，图片随机采用
+ dropout=0.5
+ Batch size = 128
+ SGD momentum = 0.9
+ Learning rate = 0.01，过一定次数降低为1/10
+ 7个CNN做ensemble：18.2%->15.4%
    
### 模型：VGGNet

**网络结构**

VGG16的网络结构，共16层（不包括池化和softmax层），所有的卷积核都使用3 * 3的大小，池化都使用大小为2 * 2，步长为2的最大池化，卷积层深度依次为64 -> 128 -> 256 -> 512 ->512。

![alt_text](./img/VGGNet.png)

**网络特点**
+ 更深的神经网络：把网络层数加到了16-19层（不包括池化和softmax层），而AlexNet是8层结构。
+ 将卷积层提升到卷积块的概念。卷积块有2~3个卷积层构成，使网络有更大感受野的同时能降低网络参数，同时多次使用ReLu激活函数有更多的线性变换，学习能力更强
+ 多使用3 * 3的卷积核：2个3 * 3卷积层 = 一层5 * 5卷积层。2层比1层更多一次非线性变换，可以学到更多东西，参数变得更少。
+ 多使用1 * 1的卷积核，起到降维的作用。
+ 每经过一个pooling层，通道数目翻倍。（信息丢失问题）

### 模型：ResNet

**加深层次存在问题：**模型深度达到某个程度后继续加深导致训练集准确率下降。

**问题解决：**假设：深层网络更难优化而非深层网络学不到东西
+ 深层网络至少可以和浅层网络持平
+ y=x，虽然增加了深度，但误差不会增加 

**网络结构**

![alt_text](./img/ResNet1.png)

+ Identity部分是恒等变换
+ F(x)是残差学习

上图为残差神经网络的基本模块（专业术语叫残差学习单元）通过不断堆叠这个基本模块，就可以得到最终的ResNet模型，理论上可以无限堆叠而不改变网络的性能。下图为一个34层的ResNet网络。
+ 先用一个普通的卷积层，stride=2
+ 再经过一个 max_pooling
+ 再经过残差结构
+ 没有中间的全连接层，直接到输出

![alt_text](./img/ResNet.png)

**网络特点**
+ 残差结构使网络需要学习的知识变少，容易学习
+ 残差结构使每一层的数据分布接近，容易学习
+ 








```{.python .input}

```
