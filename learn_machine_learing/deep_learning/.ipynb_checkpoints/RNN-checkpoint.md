# 循环神经网络RNN

+ 理解循环神经网络
+ 多层网络与双向网络
+ 模型：长短线期记忆网络（LSTM）

### 循环神经网络解决那些问题
普通神经网络一般是一个输入对应一个输出。而循环神经网络主要解决**序列式问题**。

+ 1对多：比如图片生成描述。
+ 多对1：文本分类（文本情感分析）。
+ 多对多：encoding-decoding，机器翻译、视频解说。

### RNN的特点
+ 维护一个状态作为下一步的额外输入
+ 每一步使用同样的激活函数和参数
+ 损失函数计算：每个单元的状态都会输入计算，会分块

## 分类
正向传播：

反向传播：
+ 激活函数Tanh输出在-1和1之间
+ 梯度消失
+ 较远的步骤梯度贡献很小
+ 切换其他激活函数后，而不是Tanh后，可能会导致爆炸

多层神经网络：
+ 底层输入作为高层的输出
+ 同层之间依旧递归
+ 增加网络拟合能力
+ 一般隐层维度递增
    + 64 - 128 - 256

双向网络：
+ 另一条以未来状态为输入
+ 两个状态合并后进入输入层
+ 进一步提高表达能力
+ 无法实时输出结果，需要把内容全部输入后才会有

### 为什么需要LSTM
+ 普通RNN的信息不能长久传播（存在于理论上）
+ 这时，引入了选择性机制，分为：选择性输出、选择性输入、选择性遗忘。
+ 选择性是通过门来实现的，例：Sigmoid函数：[0, 1, 0.5]

### LSTM结构
https://blog.csdn.net/gzj_1101/article/details/79376798

![alt_text](./img/rnn1.jpg)

如上图所示，从左到右：
+ 遗忘门：忘记一些信息，如，新的语句有新的主语，需要忘记掉之前的主语
+ 输入门：决定输入那些信息，如，需不需要输入主语的性别信息
+ 输出门：决定如何输出信息，如，动词该用单数形式还是复数形式
+ C t-1隐含状态传递
+ C t 当前状态

### 文本分类
**CNN文本分类：**
对于文本分类，CNN也是可以用的，句子的长度有长有短，需要做一些变换，长的取前n代表性的词，而短的做padding。如果句子很长就不生效了。
+ 一维卷积：
    + 应用在时间维度上：从句子的开头到结尾
    + Embedding长度就是通道数目
    + 多种层次的卷积核，就是不同大小的卷积核，然后得到多个结果
+ 池化： 在时间层次上pooling，每个卷积核信息的汇总
+ 全连接

**CNN和RNN的对比：**
+ CNN不能完美地解决序列式问题
+ CNN卷积相当于N-gram，只能看到n个词，LSTM提取更长的依赖
+ 双向RNN会增强效果
+ CNN模型并行程度高，更大；因为RNN需要上一次输出才能计算下一个

**CNN和RNN结合：R-CNN文本分类**
+ 双向RNN提取特征
+ CNN进一步抽取
+ Max-pooling
+ 全连接层

**Embedding压缩**
相同点：Pre-train的embedding，如果Embedding层次参数过大，模型表达能力过强就会无法使用，过拟合。

### 文本分类三个例子：在example中
+ TensorFlow中，使用LSTM实现文本分类
+ TensorFlow中，实现LSTM网络结构-
+ TensorFlow中，使用CNN实现文本分类

```{.python .input}

```
