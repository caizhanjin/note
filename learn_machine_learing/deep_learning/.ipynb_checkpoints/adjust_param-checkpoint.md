# 调参技巧

+ 各种优化算法Optimizer
+ [激活函数](./activation_function.md)
+ 网络初始化
+ 批归一化
+ 数据增强
+ 其他调参技巧


## 各种优化算法Optimizer
https://www.cnblogs.com/guoyaohua/p/8542554.html

#### 随机梯度下降
https://www.cnblogs.com/pinard/p/5970503.html

+ 定义：把求得的各个参数的偏导数以向量的形式写出来，就是梯度。从几何意义上讲，就是函数变化增加最快的地方。
+ 局部极值，只有凹函数时才确保是最优解，有时候初始值非常关键
+ Saddle point问题
+ 分类：BGD，SGD，MBGD
+ 算法调优：
    + 1. 算法的步长（Learning rate）选择。
    + 2. 算法参数的初始值选择。 
    + 3. 归一化。

#### 动量下降

随机梯度下降和动量下降存在问题：会很难学习到稀疏数据上的信息
+ 受初始学习率影响很大
+ 每一个维度的学习率一样

#### AdaGrad算法
+ 前期，分母regularrizer较小，放大梯度
+ 后期，分母regularrizer较大，缩小梯度
+ 梯度随训练次数降低
+ 每个分量有不同的学习率
+ 对稀疏的数据表现很好，减少学习率的手动调节
+ 超参数设定值：一般η选取0.01

缺点：
+ 学习率设置太大时，会导致分母regularrizer较大影响过于敏感
+ 后期时，regularrizer累积值太大，提前结束训练

#### RMSProp算法
+ AdaGrad的变种
+ 由累积平方梯度变为平均平方梯度
+ 解决了后期提前结束的问题

#### Adam算法
+ 相当于RMSprop + Momentum
+ 所有上述算法都会用learning_rate来做参数
+ 实践表明，Adam比其他适应性学习方法效果好
+ Adam比较有效的参数：
    + Beta1=0.9
    + Beta2=0.999
    + Learning_rate=1e-3 or 5e-4
    
#### 如何选择优化算法
+ 对于稀疏数据，使用学习率可自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。
+ RMSprop, Adadelta, Adam 在很多情况下的效果是相似
+ 需要训练较深较复杂的网络且需要快速收敛时，推荐使用Adam。Adam 在 RMSprop 的基础上加了 bias-correction 和 momentum，随着梯度变的稀疏，Adam 比 RMSprop 效果会好。**整体来讲，Adam 是最好的选择。**
+ 很多论文会用 SGD。SGD通常训练时间长，最终效果会比较好，但是需要好的初始化和learning rate
+ 如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。


## [激活函数](./activation_function.md)

## 一种优化算法：Batch Normalization(批归一化)
https://blog.csdn.net/wfei101/article/details/79997708


## 网络初始化
如何分析初始化结果好不好？查看初始化各层的激活值分布。

+ 1.全部初始化为0，单层网络是可以的。但是多层网络会使梯度消失，链式法则。
+ 2.均值为0，方差为0.02的正态分布初始化：tanh问题：高层均值为0，没有梯度
+ 3.均值为0，方差为1的正态分布初始化：tanh问题：高层均值为-1、1，已经饱和
+ 改善：Xavier-tanh（不适用ReLU）、He-ReLU（改进）

## 批归一化？？？
+ 每个Batch在每一层上都做归一化
+ 为了确保归一化能够起到作用，另设两个参数来逆归一化（没有效果时，就撤回来）

## 数据增强
[实例：tensorflow之图像数据增强](./data_img_enhance.ipynb)

数据层面
+ 归一化
+ 图像变换：翻转、拉伸、裁剪、变形
+ 色彩变换：对比度、亮度
+ 多尺度
    + 例子：从[256，480]中随机选择一个数，图像短边缩放到这个数，缩放后图像上裁剪(224, 224)
    
更多方法：
+ 拿到更多的数据
+ 给神经网络添加层次：逐步增加、容易发现问题，由简单到复杂
+ 紧跟最新进展，使用新方法
+ 增大训练的迭代次数：增加批次Batch，打乱数据
+ 尝试正则化
+ 使用更多的GPU来加速训练
+ 可视化工具来检查中间状态
    + 损失
    + 梯度：梯度比较小时，说明训练快结束了
    + 准确率：
    + 学习率：
    
可能存在问题：
+ 学习不到规律
+ 过拟合
+ 训练速度慢
+ 优化目标写错

## 其他调参技巧
+ 在标准数据集上训练
+ 在小数据集上过拟合
+ 数据集分布平衡
+ 使用预调整好的稳定模型结构：别人的论文
+ Fine-tuning
    + 预训练好的网络结构上进行微调
+ 加深网络

```{.python .input}

```
