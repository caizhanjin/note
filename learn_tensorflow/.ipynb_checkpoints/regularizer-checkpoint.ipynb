{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络优化几种方法\n",
    "\n",
    "+ 一种灵活学习率设置方法\n",
    "+ 正则化：regularizer\n",
    "\n",
    "### 一种灵活学习率设置方法：指数衰减法\n",
    "训练网络时，需要设置学习率（learning rate）来控制参数的更新速度。如果幅度过大，会导致找不到最优值，如果设置过小就会花费很长的时间。因此，tensorflow提供了一种灵活的设置方法：指数衰减法。`tf.train.exponential_decay`通过这个函数，可以先使用较大的学\n",
    "习率来快速得到一个比较优的解，然后随着迭代的继续逐步减小学习率，使得模型在训练后\n",
    "期更加稳定。例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0)\n",
    "\n",
    "# 通过exponential_decay函数生成学习率\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    0.1, global_step, 100, 0.96, staircase=True)\n",
    "\n",
    "# 使用指数农减的学习惑。在minimize 函数中传入global_step 将自动更新global_step参数，\n",
    "# 从而使得学习率也得到相应更新。\n",
    "learning_step = tf.train.GradientDescentOptimizer(learing_rate) \\\n",
    "            .minimize( ... my loss ... , global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化：regularizer\n",
    "http://www.cnblogs.com/linyuanzhou/p/6923607.html\n",
    "\n",
    "正则化是避免过拟合问题最常用的方法，常用的正则化有L1和L2两种。例子展示5层神经网络L2正则化的损失函数计算方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取－层神经网络边上的权茧，并将这个权重的L2正则化损失加入名称为’losses’的集合中\n",
    "def get_weight(shape, lambda):\n",
    "    # 生成一个变量。\n",
    "    var = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    \n",
    "    # add to collect 工。n 函数将这个新生成变量的L2 正则化损失项加入集合。\n",
    "    # 这个函数的第一个参数’ losses ’ 是集合的名字，第二个参数是要加入这个集合的内容。\n",
    "    tf.add_to_collection(\n",
    "        'losses', tf.contrib.layers.l2_regularizer(lambda)(var))\n",
    "    # 返回生成的变量。\n",
    "    retur var\n",
    "    \n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "batch_size = 8\n",
    "\n",
    "# 定义了每一层网络中节点的个数。\n",
    "layer_dimension = [2 , 10 , 10 , 10 , 1]\n",
    "# 神经网络的层数。\n",
    "n_layers = len(layer_dimension)\n",
    "# 这个变量维护前向传播时最深层的节点，开始的时候就是输入层。\n",
    "cur_layer = x\n",
    "# 当前层的节点个数。\n",
    "in_dimension = layer_dimension[OJ\n",
    "                               \n",
    "# 通过一个循环来生成5 层全连接的冲经网络结构。\n",
    "for i in range(1, n_layers):\n",
    "    # layer dimension[i ）为下一层的节点个数。\n",
    "    out_dimension = layer_dimension[i]\n",
    "    # 生成当前层中权重的变量，并将这个变量的L2 正则化损失J1n 入计算图上的:f.1.1 合。\n",
    "    weight= get_weight([in_dimension, out_dimension] , 0.001)\n",
    "    bias = tf.Variable(tf.constant(O.l , shape=[out_dimension]))\n",
    "    # 使用Re LU 激活函数。\n",
    "    cur_layer= tf.nn.relu(tf.matmul(cur_layer, weight) + bias)\n",
    "    # 进入下一层之前将下一层的节点个数更新为当前层节点个数。\n",
    "    in_dimension = layer_dimension[i]\n",
    "                               \n",
    "# 在定义神经网络前向传播的同时已经将所有的L2 正则化损失加入了图上的集合，\n",
    "# 这里只需要计算刻画模型在训练数据上表现的损失函数。\n",
    "mse_loss= tf.reduce_mean(tf.square(y - cur_layer))\n",
    "                               \n",
    "# 将均方误差损失函数加入损失集合。\n",
    "tf.add_to_collection('losses', mse_loss)\n",
    "                               \n",
    "# get_collectio 口返回一个列表，这个列表是所有这个集合中的元素。在这个样例1t1,\n",
    "# 这些元素就是损失函数的不同部分，将它们加起来就可以得到最终的损失函数。\n",
    "loss = tf.add n(tf .get_collection('losses'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
